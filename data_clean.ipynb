{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import transformers\n",
    "import json\n",
    "from Levenshtein import distance\n",
    "import string\n",
    "import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def get_json_value_by_key(in_json, target_key, results=[]):\n",
    "    if isinstance(in_json, dict):  # 如果输入数据的格式为dict\n",
    "        for key in in_json.keys():  # 循环获取key\n",
    "            data = in_json[key]\n",
    "            get_json_value_by_key(data, target_key, results=results)  # 回归当前key对于的value\n",
    "\n",
    "            if key == target_key:  # 如果当前key与目标key相同就将当前key的value添加到输出列表\n",
    "                results.append(data)\n",
    "\n",
    "    elif isinstance(in_json, list) or isinstance(in_json, tuple):  # 如果输入数据格式为list或者tuple\n",
    "        for data in in_json:  # 循环当前列表\n",
    "            get_json_value_by_key(data, target_key, results=results)  # 回归列表的当前的元素\n",
    "\n",
    "    return results"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "with open('data_final_with_images_filtered_include_madeup_corrected.json','r') as f:\n",
    "    data = f.read()\n",
    "    data = json.loads(data)\n",
    "sentences = get_json_value_by_key(data, 'transcript',results=[])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "salient_words = get_json_value_by_key(data, 'salient_words',results=[])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def correct_label(ref, sentences,salient_words):\n",
    "    with open(ref, 'r') as f:\n",
    "        correct_data = f.readlines()\n",
    "        for i in range(0,len(correct_data),2):\n",
    "            try:\n",
    "                sent = correct_data[i].strip('\\n')\n",
    "                correct_spans = correct_data[i+1].strip('\\n').split('\\t')\n",
    "                index = sentences.index(sent)\n",
    "                salient_words[index] = correct_spans\n",
    "            except:\n",
    "                print(i)\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "correct_label('error_labeled_sentence1.txt',sentences,salient_words)\n",
    "for i in [7012,10612,17537,18142,21794,22930]:  \n",
    "    salient_words[i] = []\n",
    "salient_words[31382] = ['sambal kangkong','spicy food','XO rice','penang rendang','nonya curry']"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "74\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Their sambal kangkong is a must try if you are a fan of spicy food! Other signature dishes include their XO rice, penang rendang and their nonya curry!'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained('vblagoje/bert-english-uncased-finetuned-pos',use_fast=False)\n",
    "salient_tokens_tag = []\n",
    "for item in salient_words:\n",
    "    if len(item) == 0:\n",
    "        salient_tokens_tag.append([])\n",
    "    else:\n",
    "        token_list_per_sentence = []\n",
    "        for span in item:\n",
    "            tokens = []\n",
    "            temperory_tokens = tokenizer.tokenize(span)   ## 作为之后查询列表的子集\n",
    "            for token in temperory_tokens:\n",
    "                if token not in '.?!,:;' and token != ' ':\n",
    "                    tokens.append(token)\n",
    "            tags = ['B']\n",
    "            tags.extend(['I' for i in range((len(tokens)-1))])\n",
    "            token_list_per_sentence.append((tokens,tags))\n",
    "        salient_tokens_tag.append(token_list_per_sentence)\n",
    "            "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "## 使用滑动窗口列表子集查询的方式，将span words的token赋值给整个句子(e.g. ['salmon', 'wrap'])\n",
    "def exact_match(target, list):\n",
    "    for i in range(len(list)-len(target)+1):\n",
    "        if target == list[i:i+len(target)]:\n",
    "            return i\n",
    "            break\n",
    "\n",
    "def stripping(list):\n",
    "    str1 = ''\n",
    "    for item in list:\n",
    "        if item.startswith('##'):\n",
    "            str1 += item[2:]\n",
    "        else:\n",
    "            if str1 == '':\n",
    "                str1 = item\n",
    "            else:\n",
    "                str1 += item\n",
    "    for i in '.?!,:;':\n",
    "        str1 = str1.replace(i, '')\n",
    "    str1.strip()\n",
    "    return str1\n",
    "    \n",
    "def approximate_match(target, list):\n",
    "    str1 = stripping(target)\n",
    "    for i in range(len(list)):\n",
    "        for j in range(i+1,len(list)):\n",
    "            str2 = stripping(list[i:j])\n",
    "            if distance(str1,str2) <= 1:\n",
    "                return i,j  #返回错误的span对应句子中正确的位置\n",
    "                break\n",
    "\n",
    "final_output = []\n",
    "error_sentence_index = []\n",
    "assert len(sentences) == len(salient_tokens_tag)\n",
    "for i in range(len(sentences)):\n",
    "    salient_reference = salient_tokens_tag[i]\n",
    "    tokenized_sentence = tokenizer.tokenize(sentences[i])\n",
    "    token_labels = ['O' for i in range(len(tokenized_sentence))] \n",
    "    if len(salient_reference) > 0:\n",
    "        for (span, span_label) in salient_reference:\n",
    "            position = exact_match(span, tokenized_sentence)\n",
    "            if position == None:\n",
    "                try:\n",
    "                    position, end = approximate_match(span, tokenized_sentence)\n",
    "                    span = tokenized_sentence[position:end]  ## 获得正确的span\n",
    "                    span_label = ['B']\n",
    "                    span_label.extend(['I' for i in range((len(span)-1))])\n",
    "                except:\n",
    "                    pass\n",
    "            if position != None:\n",
    "                token_labels[position:position+len(span)] = span_label\n",
    "            else:\n",
    "                error_sentence_index.append(i)\n",
    "    final_output.append((tokenized_sentence, token_labels))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# double check coherence between label and sentence\n",
    "for sentence, label in final_output:\n",
    "    if len(sentence) != len(label):\n",
    "        print(final_output.index((sentence, label)))\n",
    "        # here should be no output printed"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# combine adjacent two sentences into one turn, with addition of [CLS] and [SEP] tokens\n",
    "assert len(final_output) % 2 == 0\n",
    "output_per_turn = []\n",
    "for i in range(0,len(final_output),2):\n",
    "    sentence_turn = ['[CLS]']+final_output[i][0]+['[SEP]']+final_output[i+1][0]+['[SEP]']\n",
    "    label_turn = ['O']+final_output[i][1]+['O']+final_output[i+1][1]+['O']\n",
    "    output_per_turn.append((sentence_turn, label_turn))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "## split data into train, dev and test\n",
    "def split_data(full_list, ratio = [0.8,0.1,0.1], seed = 0):\n",
    "    random.seed(seed)\n",
    "    assert sum(ratio) == 1\n",
    "    cutpoint1 = round(len(full_list) * ratio[0])\n",
    "    cutpoint2 = round(len(full_list) * sum(ratio[:2]))\n",
    "    random.shuffle(full_list)\n",
    "    train_set = full_list[:cutpoint1]\n",
    "    dev_set = full_list[cutpoint1:cutpoint2]\n",
    "    test_set = full_list[cutpoint2:]\n",
    "    with open(f'dataset/train_set.json','w') as g:\n",
    "        json.dump(train_set, g)\n",
    "    with open(f'dataset/dev_set.json','w') as g:\n",
    "        json.dump(dev_set, g)\n",
    "    with open(f'dataset/test_set.json','w') as g:\n",
    "        json.dump(test_set, g)\n",
    "\n",
    "split_data(output_per_turn)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "## check the saved data\n",
    "import json\n",
    "with open('input_data.json','r') as f:\n",
    "    data = f.read()\n",
    "    data = json.loads(data)\n",
    "for i in range(len(data)):\n",
    "    if len(data[i][0]) != len(data[i][1]):\n",
    "        print(i)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "def get_max_length(file):\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.loads(f.read())\n",
    "        max_len = 0\n",
    "        for sentence, label in data:\n",
    "            if len(sentence) > max_len:\n",
    "                max_len = len(sentence)\n",
    "    return max_len\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "171  213  153"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "output_per_turn[1]"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'output_per_turn' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ef11c3f22690>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput_per_turn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'output_per_turn' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.0",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.0 64-bit ('python37': conda)"
  },
  "interpreter": {
   "hash": "a93911f085a41939b3af287750bd31cf8d3acebdeaefd56173dac79d56f5bd9f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}